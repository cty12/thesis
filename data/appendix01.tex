\chapter{外文资料的调研阅读报告或书面翻译}

\title{Xen 与虚拟化的艺术}

{\heiti 摘要：} 人们设计了无数的系统以通过虚拟化技术分享现代计算机的充足的计算资源。
其中，有的系统需要特制的硬件，或者不能运行日常使用的操作系统；有的为了百分之百的二进制兼容
不得不以性能作为代价；还有的为了性能好牺牲了安全性和功能性。只有很少的一部分提供资源隔离和
性能的保障，大多数只提供“尽力而为”的表现，承担着很高的拒绝服务风险。

在本文中，一个叫做 Xen 的 X86 虚拟化管理程序被提出出来。它使多个常见的操作系统可以以安全、
资源可控而且性能功能都有保障的方式，共享传统的硬件设备。这是通过提供理想化的虚拟机抽象达到的，
常见的操作系统例如 Linux 、 BSD 还有 Windows XP 可以只需很少的努力移植到这个平台。

我们的设计指标允许多达 100 台虚拟机实例在一台物理服务器主机上同时运行。我们采用的虚拟化方法
是非常高效的，例如 Linux 和 Windows XP 的操作系统可以在我们的平台上以极少的性能开销共同
运行——相较于直接运行在物理机上只有百分之个位数的开销。无论在微基准测试，还是在全系统的测试上，
我们的系统都极大地超越了与其竞争的商业和免费的现有解决方案。

\section{引言}

现代的计算机足够强大，可以通过虚拟化技术启动许多小的“虚拟机”，并在每个虚拟机上运行一个独立的
操作系统实例。这导致了虚拟化技术研究兴趣的回潮。在本文中，我们提出了 Xen ，一个高性能、资源
可控的虚拟机管理程序，它可以支持服务器联合、协同托管服务、分布式 web 服务、安全计算平台和
应用程序移动性等多种应用。

成功地将一台主机划分成并行执行的多个操作系统具有以下几个挑战：首先，虚拟机必须保证互相隔离，
它们的执行过程互相影响是不能接受的。尤其是在虚拟机被多个互相不信任的用户共同使用的情况下。
其次，必须支持多种操作系统，才能适应流行的应用程序的多样性。最后，虚拟化带来的开销应该尽可能小。

Xen 可以承载常见的操作系统，尽管需要进行源代码的更改。本文中描述和评估的 Xen 样本支持多个
并行执行的 XenoLinux 客户操作系统实例，每个实例向上对应用程序提供和非虚拟化的 Linux 2.4
完全相同的二进制接口。我们对 Windows XP 的移植虽然是不完整的，但是已经可以运行简单的用户态
应用程序。我们还在移植 NetBSD 操作系统。

Xen 允许用户动态地实例化一个操作系统，使其可以在用户希望的任何地方运行。在 XenoLinux
项目中，我们在互联网提供商和互联网交换机的较为经济的位置上的物理节点上部署了 Xen 系统。
我们在它开启新的虚拟机前进行了准入控制，并且希望每个虚拟机为它所需的资源付出相应的“代价”。
我们会在另外的地方从这个角度讨论我们的想法和实现方法，本文更多地还是关注虚拟机管理程序本身。

在一个共享的物理系统上部署多种应用程序和服务有很多种方法，其中很可能是最简单的一种就是部署
多个主机，每个运行一个标准的如 Linux 或者 Windows 的操作系统，然后用户可以安装文件、
开启程序，应用程序之间的保护机制是由传统的操作系统技术实现的。经验表明，如果这样做，系统
管理工作很快就会变成一项耗时的任务，因为本该互相隔离的应用程序之间复杂的交互配置。

更重要的是，这种系统支持的性能隔离还不够——一个进程的调度优先顺序、内存需求、网络流量还有
磁盘访问会直接影响其它程序的。对于事先规划好的固定的用户群体，这个也许是可以接受的（比如
网格计算或者实验性的 PlanetLab 平台），但是如果资源被过度订阅，或者用户互相协作不愉快，
那么这个模式就不可行。

一个解决方式是翻新操作系统本身对于性能隔离的支持。Resource container 技术、Linux/RK 、
QLinux 和 SILK 或多或少都表现出这个思想。一个主要的困难是保证所有资源占用的帐都记到正确的
那个进程的头上——尤其是考虑到例如缓冲区缓存或者页置换算法导致的应用程序之间复杂的交互。在
操作系统内部这是被称为“QoS crosstalk“的问题。降低复用率可以缓解这个问题，正如 Exokernel
和 Nemesis 操作系统展示的那样。在它们的实现中非刻意的或者不需要的进程间交互被降低到了最低。

我们使用类似的基本方法来创造 Xen ，在 Xen 中物理资源被以整个操作系统的粒度进行复用，
并且能提供它们之间的性能隔离。和进程级的复用不同的是，这种复用允许多个不同的客户操作系统
优雅地共同存在，而不是使用同一个应用层二进制接口。当然，这样的灵活性肯定要付出代价——运行整个
完整的操作系统肯定比运行单个进程更加“重量”，无论是从开机启动来说，还是从资源消耗来讲。

对于我们的目标也就是多达 100 个承载的虚拟机实例，我们相信上述代价是值得的，因为它单个用户
以资源可控制的方式运行未经修改的二进制或者一族二进制程序，例如一个 Apache 服务再加上一个
PostgreSQL 后端）。另外，它还提供了极致的灵活性，因为用户可以精确地恰到好处地创建那个
应用需要的运行环境。麻烦的在多个服务和应用之间的配置协调被避免了（比如说，每个 Windows 实例
可以维护自己独立的注册表）。

本论文的如下部分这样组织：在第二章我们介绍实现虚拟化的具体方法和 Xen 的基本工作原理。
在第三章介绍我们的设计和实现的一些关键细节。在第四章我们使用工业界标准的基准测试程序对在
Xen 上运行的 XenoLinux 内核与在物理机上运行的 Linux 内核、VMWare Workstation 还有
用户态 Linux (UML) 进行比较测试。第五章评价相关工作。第六章介绍后续工作和结论。

\section{Xen: 方法与概述}

在传统的虚拟机管理程序里，虚拟化软件对上层暴露出和下层的硬件完全相同的功能。尽管全虚拟化
有着显而易见的优点，也就是允许未经修改的操作系统在上面运行，但是它有几个缺点。对于流行的
IA-32 / X86 架构来说，更是如此。

对于全虚拟化的支持从来就不是 X86 体系结构的一部分。特定的内核指令必须经过虚拟机管理程序的
处理才能实现正确的虚拟化，但是在不足的权限下运行这些指令会悄无声息地失败，而不是导致易于
处理的陷入。高效率地将 X86 的 MMU 进行虚拟化也是非常困难的。这些问题当然都可以被解决，
只不过需要以增加的复杂度和降低的效率作为代价。VMWare 的 ESX Server 自动地重写一部分客户机
的代码来插入陷入，每当虚拟机管理程序必须介入执行的时候。这个翻译过程必须应用到整个客户机的
操作系统内核上（带来相应的翻译、执行、缓存的开销）因为所有不会带来陷入的特权指令必须被
捕获和处理。ESX Server 还实现了系统数据结构比如页表的一个“影子”版本，并且通过陷入每个
更新操作维护虚拟页表的一致性——这个实现方法对于更新频繁的操作，比如创建一个新的应用程序进程
带来了非常大的开销。

难以忍受 X86 错综复杂的特性，存在其它反对全虚拟化的讨论。特别是有客户虚拟机想要既能看到真实
的硬件资源又能看到虚拟的硬件资源的实际的应用情况，例如既提供实际的时钟又提供虚拟的时钟可以使
客户虚拟机更好地支持时间敏感的任务，使其准确地处理 TCP 超时以及 RTT 估计，又比如暴露出
物理机的实际地址允许客户机通过使用 superpages 和 page coloring 技术获得更好的性能表现。

我们通过提供一个和下层的硬件类似但是又不完全相同的虚拟机抽象来避免全虚拟化的缺陷——这种方法
被称为“半虚拟化”。这种方法会带来更高的性能，尽管需要修改客户机的操作系统。但是请注意，
我们不需要修改应用程序的二进制接口 ABI ，所以上层应用无须做修改。

我们将上述讨论总结为如下几个原则：

\begin{enumerate}
    \item 支持未经修改的应用程序二进制文件至关重要，否则用户就不会迁移到 Xen 。
    因此我们必须将现有的 ABI 的全部结构特性进行虚拟化；
    \item 支持完整的多任务操作系统是重要的，因为这样可以允许复杂的服务设置在单一的客户机
    操作系统实例内被虚拟化；
    \item 在 X86 这种虚拟化不友好的平台上，半虚拟化是必须的，因为通过它可以获得高性能
    和强的资源隔离；
    \item 即便在虚拟化友好的硬件架构上，完全地对客户操作系统隐藏虚拟化的效用也会带来正确性
    与性能的风险。
\end{enumerate}

注意，我们的 X86 半虚拟化抽象和最近的 Denali 项目提出的设计非常不同。Denali 被设计用来
支持数以千计的运行网络服务的虚拟机，它们中大多数都规模不大并且也不流行。形成对比的是，Xen
的设计目标是支持多达 100 个运行行业标准的应用与服务的虚拟机。因为目标非常不同，将 Denali
的设计方法和我们的原理作对比是有益的。

首先，Denali 的目标不在于已有的 ABI ，所以可以从虚拟机接口中忽略一部分结构特性。
举例来说，Denali 并不完全支持 X86 的分段机制，尽管它在 NetBSD 、Linux 和 Windows XP
的 ABI 中广泛使用。

其次，Denali 的实现方法没有解决支持应用程序复用或者一个客户操作系统的不同的地址空间的问题。
应用程序被以类似 Exokernel 中的 libOS 的形式显式地链接向一个 Ilwaco 客户操作系统的实例。
因此，每个虚拟机只能承载一个单用户模式的单应用的未经保护的所谓“操作系统”。形成对比的是，
在 Xen 中，一个虚拟机可以承载一个真正的操作系统，在这个操作系统上又可以复用出成千上万的
未经修改的用户态进程。尽管一个原型虚拟 MMU 被开发出来用于帮助 Denali 达到类似的目标，
但是我们没有看到任何已发表的技术细节或者评估报告。

再次，在 Denali 的架构中虚拟机管理程序通过磁盘输入输出操作所有分页机制。这可能是因为在
虚拟化层缺少内存管理机制。在虚拟机管理程序内部解决分页机制和我们的进行性能隔离的目标向左：
恶意的虚拟机会激励内存抖动，不公平地把别的虚拟机的 CPU 时间和磁盘带宽据为己有。在 Xen 系统中
我们希望每个客户机都有自己的分页机制，使用自己分配到的那一份内存空间和磁盘份额。

最后，Denali 将每台机器的资源的命名空间进行了虚拟化，采用了如果一台虚拟机不能对另一台虚拟机
的资源进行命名那么就无法访问该资源的方式。举例来说，虚拟机们对硬件地址一无所知，它们只知道
Denali 为它们创建的虚拟地址。相反，我们认为虚拟机管理程序内部的安全访问管理对于保护来说
已经足够了，另外，正如上文讨论过的，如果使物理资源被客户操作系统直接访问会带来很激烈的正确性
与性能的争论。

在下面的一节中我们讨论 Xen 的虚拟机抽象，并讨论 Xen 如何通过修改客户操作系统来达到这一点的。
注意，在本文中，我们使用术语“客户操作系统”来指代 Xen 可以承载的操作系统之一，使用“域”来指代
一个运行着某个“客户操作系统”的虚拟机；这个区别好似在传统的系统中程序与进程的概念。我们管 Xen
本身叫做虚拟机管理程序 (hypervisor) 因为它执行的特权级相对于它承载的客户操作系统的内核更高。

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{the-paravirt-x86-interface}
\end{figure}

\subsection{虚拟机接口}

表 1 展示了 X86 半虚拟化接口的一个概览，主要可以划分为三个大块：内存管理、CPU 还有
输入输出设备。在下面的章节我们依次介绍三个子系统，并讨论每个部分是如何在我们的半虚拟化架构中
被表示的。注意，尽管我们的实现中的一部分，比如内存管理，是特定地针对 X86 平台的，许多方面
例如虚拟 CPU 和输入输出设备可以直接应用到别的体系结构上。另外，X86 体系结构代表了它区别于
RISC 体系结构的最坏的一种可能性，比如说，有效地将硬件页表虚拟化比将软件控制的 TLB
虚拟化要更加复杂。

\subsubsection{内存管理}

将内存虚拟化毫无疑问是将体系结构半虚拟化中最困难的部分，无论是从虚拟机管理程序的原理上讲，
还是从每种客户操作系统被移植的工作量上讲。如果体系结构本身提供一个软件管理的 TLB 那么这个
工作就更加容易了，因为这样的结构能简单有效地被虚拟化。TLB 标记是另一种被多数服务器级的 RISC
体系结构支持的有用的功能，比方说 Alpha 、MIPS 还有 SPARC 。将地址空间标志位同每个 TLB
表项联系起来使得虚拟机管理程序与每个客户操作系统有效地在分开的地址空间里共同存在，因为在二者
转换执行的时候就没有必要清空掉整个 TLB 了。

遗憾的是，X86 并没有软件控制的 TLB ，与此相反的是 TLB 缺失被 CPU 本身以遍历整个页表结构
的形式自动处理。因此为了达到最好的性能，所有有效的对于当前的地址空间内的页地址转换都需要在硬件
可访问的页表中存在。另外，因为 TLB 没有标记，地址空间转换通常需要清空整个 TLB 。基于这些限制，
我们作出了两个决定：(1) 客户操作系统负责分配和管理硬件页表，Xen 的介入非常至少来保证安全性
和隔离性；(2) Xen 在每段地址空间的上 64 MB 存在，因此避免进入退出虚拟机管理程序造成的 TLB
清空。

每次一个客户操作系统需要一个新的页表——也许是因为一个新的进程创建了——它从它自己的内存保留
区域内分配并初始化一页并在 Xen 上做登记。此时，操作系统必须放弃对页表所在的内存区域直接写入
的权限，之后所有的更新都需要被 Xen 批准。更新在几个方面被限制，包括只允许操作系统对它拥有的页
进行映射，并且禁用页表的可写入映射。客户操作系统可以通过一次送入一批更新请求的方式来减少进入
虚拟机管理程序的开销。每个地址空间上方 64MB 预留给 Xen ，对于客户操作系统是不可访问或者
重新映射的。这个地址区域不被任何常见的 X86 ABI 使用，所以这个限制不会影响应用程序的兼容性。

分段被用类似的方式进行虚拟化，就是批准对硬件段描述符表的更新。X86 里的段描述符惟独的限制是：
(1) 它们的特权级比 Xen 低；(2) 它们不允许任何对 Xen 预留的地址空间的访问。

\subsubsection{CPU}

对 CPU 进行虚拟化对于客户操作系统来说有几个含义：首先，在客户操作系统下方插入虚拟机管理程序
和传统意义上的“操作系统才是特权最高的”的概念相违背。为了保护虚拟机管理程序免于操作系统的
异常行为（以及域与域之间的隔离），客户操作系统必须进行修改在更低的特权级上运行。

很多的处理器体系结构只提供两个特权级。在这种情况下，客户操作系统会与应用程序一起共享较低的
特权级。客户操作系统通过在与应用程序不同的地址空间里运行而把自己保护起来，通过虚拟机管理程序
设置虚拟的特权级和改换当前地址空间从而间接地与应用程序交替运行。如果处理器的 TLB 支持地址空间
标记，那么代价昂贵的 TLB 清空就可以被避免了。

高效地将特权级进行虚拟化在 X86 上是可行的，因为它在硬件层面支持四个不同的特权级。X86 的特权级
一般被称为 rings ，从 0 到 3 。操作系统的代码通常在 ring 0 执行，因为其它的 ring 都不能
执行特权指令，而 ring 3 一般被应用程序的代码使用。据我们所知，ring 1 和 ring 2 自从 OS/2
就没有被任何著名的 X86 操作系统使用过。任何符合这个特点的操作系统都可以被移植到 Xen 上通过
修改以在 ring 1 里运行。这可以防止客户操作系统直接运行特权指令，并且可以和 ring 3 上运行的
应用程序隔离开来。

特权指令通过在 Xen 内部批准和执行来半虚拟化。这些适用于类似创建一个新的页表或者在空闲时放弃
CPU 资源等操作。任何客户操作系统的直接执行特权指令的尝试都会被处理器本身拒绝掉，或者是静默地
或者产生一个错误，因为只有 Xen 本身才能在一个权限足够的层级运行。

异常，包括内存异常和软件的陷入，在 X86 上的虚拟化都十分直接。描述每种异常的处理程序的表在
Xen 上登记待批准。这些在表中声明的处理程序基本上和对于真实 X86 硬件的相同。这是可行的，
因为在我们的半虚拟化架构中异常的栈帧没有被修改。唯一的修改是对于缺页异常的处理程序，它通常是
从特权寄存器 CR2 中读取缺页的地址，但是因为这是不可能的，我们把它写在了一个扩展的栈帧里。
每当一个异常发生在在 ring 0 外执行，Xen 的处理程序创建客户操作系统上异常栈帧的一个拷贝，
然后把控制权交给注册的处理程序。

通常，只有两类异常的发生会频繁到影响系统性能的地步：系统调用（它们通常是通过软件异常实现的）
还有缺页异常。我们通过允许每个客户机操作系统注册一个“快的”异常处理程序，它可以被处理器直接访问
而不是间接通过 ring 0，来提高系统调用的性能。这个处理程序在被写到硬件异常表中之前就被有效化了。
遗憾的是，不能对缺页处理程序使用相同的技术，因为只有 ring 0 里执行的代码才能访问 CR2 里
存储的缺页地址，缺页异常必须通过 Xen 被传送一次，使得寄存器的值能从 ring 1 中访问。

安全性是由直到异常处理程序被传给 Xen 才被有效化而保证的。唯一需要的检查是处理程序的代码段
不会提出去 ring 0 执行。因为没有客户操作系统能创建类似的段，它有能力比较那个段选择子与 Xen
预留的几个静态的值之间的大小关系。除此之外，其它的一些处理程序的问题会在异常的传输过程中被解决
——举例来说，如果处理程序的代码段不存在或是处理程序没有被进行内存分页，那么当 Xen 执行 iret
指令返回处理程序的时候，相应的异常就会被处理。Xen 通过检查异常程序的计数器来检测“双误”
(double faults) ，如果地址处于异常虚拟化的代码处，那么触犯的客户操作系统会被终止。

请注意，即使对于直接的系统调用处理程序，惰性检查也是安全的，因为访问异常只有当 CPU 尝试直接
跳转到客户操作系统的处理程序时才会发生。在这种情况下，异常的地址是在 Xen 之外的，因为 Xen
永远也不会调用客户操作系统的系统调用，所以异常被虚拟化的方法是正常的。如果异常在传导过程中
导致了双误，那么客户操作系统会被以如上方式终止。

\subsubsection{设备 I/O}

和全虚拟化环境中模拟现实存在的硬件设备不同的是，Xen 暴露出的是清晰而简单的设备抽象。
这就允许我们设计出一个既高效又满足对于保护和隔离的需要的接口。为此，I/O 数据通过 Xen
在各个域之间传递，使用的是共享内存的、异步的缓冲区描述符的环 (buffer-descriptor rings) 。
这就提供了一个高效的在系统中垂直传递缓冲区信息的通讯机制，Xen 可以借此实现高效率的有效性检查，
比如检查包含在域的保留内存内的缓冲区）。

与硬件中断相似，Xen 支持一种轻量的传送机制，它被用来向一个域发送异步的通知。这些通知是通过
更新待完成的事件类型的一个位映射实现的，也可以选择使用调用由客户操作系统指明的事件处理程序。
这些事件的回调可能会因为客户操作系统的决策而被暂缓——比如说为了减少因为频繁的唤醒导致的额外的开销。

\subsection{向 Xen 移植操作系统的开销}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{xen-porting-simplicity}
\end{figure}

表 2 通过代码行数展示了向 Xen 的半虚拟化环境移植常见操作系统的代价。注意我们的 NetBSD 移植
尚处在非常初始的阶段，所以没把它的数据加入表格。Windows XP 的移植处于更高级的阶段，但是也尚
未完工，它可以从 ramdisk 中执行一些用户态的程序，但是全部虚拟 I/O 都还不好使。正因如此，所有
XP 的虚拟设备驱动的数据都没有在图表中出现。但是对于 Linux 来说，我们认为这些驱动都是短小精悍
的，因为 Xen 提供了理想化的硬件抽象。

Windows 令人惊讶地需要极大量对于机器无关代码部分的修改，因为它访问页表项 PTE 使用了很多种
不同的数据结构与联合。每种页表的访问都需要被单独修改，尽管一部分可以通过脚本来自动化。恰恰相反
的是 Linux 需要的对于内存系统的修改相对来说非常少，因为它使用预处理的宏来访问 PTE ——往预处理
宏内加入半虚拟化需要的翻译和虚拟机管理程序的调用就要方便多了。

在两种操作系统中，体系结构相关的部分都是我们从 X86 移植到半虚拟化要显著改写的。这包括改写
使用了特权指令的例程，还有移除大量的底层的系统初始化代码。再一次，Windows XP 需要的工作量更大，
主要是因为遗留的 16 位模拟层的存在以及需要一个不同的启动-加载机制。主要到在 Windows XP 中
X86 特有的代码比 Linux 里的要显著地更多，所以更大的移植的工作量是可以预见到的。

\subsection{控制与管理}

Xen 的设计与实现从始至终的一个目标就是将策略 (policy) 与机理 (mechanism) 尽可能地分离开。
尽管虚拟机管理程序必须涉及进数据路径的方面，比如在域和域之间切换 CPU 占用，在传输之前过滤
网络的包，或者是在读取数据块的时候保障访问权限控制，它没有必要涉及进更高层的事情，比如 CPU
是如何共享的，或者每个域可能会传输怎样的数据包。

这导致了一个虚拟机管理程序本身仅仅提供基本的控制操作的架构。这些操作通过能被从授权的域访问的
接口导出到上层。潜在复杂的策略，比如准入控制，最好是由在客户操作系统中运行的管理软件来完成，
而不是在特权的虚拟机管理程序的代码里做。

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{xen-arch-orig}
\end{figure}

整体上的系统架构如图 1 所示。注意一个域在启动时被创建，允许使用控制接口。最初的域，叫做
Domain0 ，负责运行应用层的管理软件。控制接口提供了创建和终止其它域的能力，并且能控制他们关联
的调度的参数、物理内存分配还有物理机的磁盘和网络设备配给的访问权限。

除了处理器和内存资源之外，控制接口还支持虚拟网络接口 (VIF) 和块设备 (VBD) 的创建与删除。
这些虚拟的 I/O 设备具有关联的访问控制信息，决定了哪个域能访问它们，并且受到哪些限制。举例来说，
一个只读的 VBD 可能被创建，或者一个 VIF 可能会通过过滤 IP 包来防止源地址伪造。

控制接口与当前系统状态的统计信息，导出到一个在 Domain0 中运行的应用层管理软件套件中。这个对于
管理工具的补足使得方面地管理整个服务器成为可能：当前的工具可以创建或者销毁域、设置网络过滤器和
例程的规则、在数据包和数据流的粒度监控每一个域的网络状态、创建和删除虚拟网络接口和虚拟的块设备。
我们预期对高层的工具的开发会使得管理策略应用的自动化更进一步。

\section{设计的细节}

在本章中我们介绍组成基于 Xen 的服务器的几个主要的子系统。为了展示得清楚，在每个情况里我们既
展示 Xen 的也展示客户操作系统的功能。目前对于客户操作系统的讨论集中在 XenoLinux 身上，因为
它是当前最为成熟的。尽管如此，我们接下来的对 Windows XP 以及 NetBSD 的移植工作给了我们 Xen
是客户操作系统无关的信心。

\subsection{控制的传输——Hypercall 与事件}

Xen 与它上面的域存在两个控制交互的机制，同步的从域到 Xen 的调用可以通过 hypercall 实现，
而从 Xen 到域的通知 (notification) 则是使用一个异步的事件 (event) 机制实现。

Hypercall 的接口使得域能够产生一个进入到虚拟机管理程序的同步的软件陷入以进行特权操作，类似于
传统的操作系统里的系统调用的作用。一个 hypercall 的实例应用是要求一系列页表的更新，其中 Xen
批准和应用了一系列更新，在完成之后返回调用的域继续执行。

从 Xen 到域的通讯是由异步的事件机制实现的，它取代了传统的设备终端的传送机制，允许重要事件使用
轻量的通知 (notification)，例如终止一个域的请求。类似于传统的 Unix 的信号 (signal) 机制，
只有少数几个事件，每个标志一种特殊类型的出现。举例来说，事件被用来显示网络传来了新数据，或者
一个虚拟磁盘请求完成了。

暂缓的事件存储在每个域都有的一个位映射区域内，在唤起由客户操作系统指明的事件回调处理程序之前
由 Xen 进行更新。回调处理程序负责重置暂缓事件的集合，还负责以正确的方式响应通知
(notification) 。域可以通过设置 Xen 可读的软件标记来显式地推迟事件的处理，这与在真实的
处理器上关闭中断的办法是相似的。

\subsection{数据的传输——I/O 环 (rings)}

虚拟机管理程序的存在意味着在客户操作系统与 I/O 设备之间存在着额外的防护，所以数据传输机制
能够保障数据在系统之中垂直以很小的开销传输就至关重要。

我们的 I/O 传输机制是由两个关键因素决定的——资源管理和事件通知。对于资源的可描述性，我们尝试
最小化分离当中断从设备被接受到时候去向一个特定的域的数据所需的工作——管理缓冲区的开销直到计算
任务被分配给恰当的域的时候才会产生。类似的，设备 I/O 占用的内存是每当防止共享的缓冲区的内在
串扰是可行的的时候，从相关的域提供出来的。I/O 缓冲区在数据传输的时候通过与 Xen 绑定下层的
页帧保护起来。

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{io-rings}
\end{figure}

图 2 展示了我们的 I/O 描述符的环的结构。一个“环”就是一个由被域分配的描述符组成的环形队列，
它从 Xen 内部是可以访问的。这些描述符并不直接包含 I/O 数据，相反，I/O 数据缓冲区是由客户
操作系统在外面分配并间接被 I/O 描述符引用的。对于每一环的访问是基于两对生产者-消费者指针的：
域在一个环上放置请求，将请求生产者指针加一，然后 Xen 将请求取出进行处理，将一个关联的请求
消费者指针加一。请求的回复也以类似的方式放置回环上，保存成以 Xen 作为生产者、以客户操作系统
作为消费者的模式。请求不需要被顺序地处理，因为客户操作系统将每个请求关联一个唯一的标识符，
这也就导致了关联过的回复。这使得 Xen 因为调度或者优先级的考虑清楚地重排 I/O 操作成为可能。

这个结构足矣支持多个不同的设备范式。举例来说，一系列请求可以为网络包接收提供缓冲区，后续的回复
标志着数据包进入这些缓冲区。处理磁盘请求时重排序是有用的，因为它允许请求在 Xen 内部为了高效率
而进行调度，与外部的缓冲区一起使用描述符使得实现零拷贝 (zero-copy) 传输变容易了。

我们将请求与回复的产生与其它部分的通知解除组对：在请求的情况下，一个域可能会在唤起一个通知
Xen 的 hypercall 之前入队许多项；在回复的情况下，一个域可以通过指定一个回复数的阈值来推迟
通知事件的发送。这使得每个域可以在延迟与吞吐率之间做折衷，这和 ArseNIC 千兆以太网接口中的
flow-aware 中断分配是类似的。

\subsection{子系统虚拟化}

上面描述的控制和数据传输的机制在我们的虚拟化平台的不同子系统中都有应用。在下文中，我们讨论
虚拟化对于 CPU 、时钟、内存、网络和磁盘分别是如何实现的。

\subsubsection{CPU 调度}

Xen 当前使用 BVT (Borrowed Virtual Time) 算法来实现不同域之间的调度。我们选择这个算法
是因为它需要的工作量比较少，另外有一个域接收事件唤醒时候的低延迟的特殊机制。对于最小化在设计
为以适时的方式运行的操作系统子系统上虚拟化的影响来说，快速的分配 (dispatch) 尤为重要。举例
来说，TCP 依赖及时的确认 (acknowledgement) 来正确估计网络来回的时间。BVT 使用虚拟时间扭曲
(virtual-time warping) 提供了低延迟的分配，这是一种临时破坏“理想地”公平的分配方式来偏向
于最近唤醒的域的分配机制。然而别的调度算法也可以通过我们的调度器抽象简单地实现。每一域的调度
参数可以通过运行在 Domain0 里的管理程序来调节。

\subsubsection{时间与时钟}

Xen 为客户操作系统提供了真实时间、虚拟时间、挂钟时间 (wall-clock time) 的概念。真实时间
表示成机器启动后过去的纳秒数量，以进程的循环计数器的精度被维护，而且可以频繁地与外部时间源进行
锁定，例如通过 NTP 。一个域的虚拟时间只有当它在执行的时候才会前进，这通常是用于客户操作系统的
调度器保证它的时间片能被应用进程正确地共享的。最后，挂钟时间定义为当前的真实时间加上的一个
偏置量。这保证了挂钟时间能在不影响真实时间的前进的情况下被调节。

每一个客户操作系统可以编程一对闹钟计时器 (alarm timer) ，一个用于真实时间，一个用于虚拟时间。
客户操作系统应该维护内部的计时器队列并且使用 Xen 提供的闹钟计时器来触发最早的超时。超时用
Xen 的事件机制进行传递。

\subsubsection{虚拟地址转换}

和其它子系统一样， Xen 试图将内存访问以尽可能小的开销进行虚拟化。正如在 2.1.1 小节讨论的，
这个目标被 X86 架构的硬件页表的应用变得更加复杂了。VMWare 采取的方式是提供给每个客户操作系统
一个虚拟的页表，而且是 MMU 不可见的。虚拟机管理程序负责陷入访问虚拟页表，批准更新，并在虚拟
页表和 MMU 可见的“影子”页表之间不断传递更改。这极大地增加了某些客户操作系统操作的开销，
比如创建虚拟地址空间，并且要对硬件更新“访问”和“修改”位进行显式的传导。

尽管全虚拟化强迫必须使用影子页表来提供连续物理地址的“幻觉”，Xen 并不是这样受限的。其实 Xen
只需要参与页表的更新操作就可以了，以避免客户操作系统作出不可接受的改动。因此我们就可以避免使用
“影子页表”带来的额外的复杂性和开销——Xen 的方法是将客户操作系统的页表直接注册到 MMU ，并且把
客户操作系统的权限限制为只读 (read-only) 。页表的更新是通过 hypercall 传递给 Xen 的，
为了保障安全性，请求在应用之前必须先经过批准。

为了实现批准的过程我们将每个机器页帧和一个类型与引用计数关联起来。一个帧在每一个时间节点上只能
是下述的几个互斥的类型其一：页目录 (PD) 、页表 (PT) 、本地描述符表 (LDT) 、全局描述符表
 (GDT) 或者可写 (RW) 。注意到一个客户操作系统总是为自己拥有的页帧创建可读的映射，无论它们
当前的类型是哪一个。一帧直到它的引用计数减到 0 才能被安全地重新分配任务 (retask) 。这个
机制用来维护安全所需要的不变性，比方说，一个域不能对页表的任何部分做可写的映射，因为这要求
相关的那一帧同时为 PT 和 RW 类型。

这个类型系统还被用来跟踪哪一帧被批准在页表中使用。客户操作系统将一帧分配给页表使用的时刻指示
出来——这就需要 Xen 批准一帧中的每个项目，在那以后它的类型被以恰当的方式绑定到 PD 或者 PT 上，
直到客户操作系统接下来发出解绑的请求。这对于当改换页表基地址的时候特别有用，因为它消除了在每次
上下文切换的时候批准新页表的需求。注意一帧不能被重新分配任务 (retask) 直到它既被解绑而且
它的引用计数减少到 0 ，这就阻止了客户操作系统使用解绑请求来绕过引用计数机制。

为了尽量减小 hypercall 的数量，客户操作系统可以在使用单一的 hypercall 应用一系列更新之前，
先在本地维护一个这些更新构成的队列——这在创建新的地址空间的时候大有裨益。但是我们必须保证更新
提交得足够及时来保障正确性。幸运的是，客户操作系统通常会在第一次使用新的映射之前进行一次 TLB
清空操作，这就保证了任何缓存的地址转换都被无效化了。因此，在 TLB 清空之前立即提交缓存的更新
操作通常能够满足正确性要求。尽管如此，有的操作系统在它确定 TLB 里没储存过时条目的时候省略了
清空操作。在这种情况下，使用新的地址映射的第一次请求可能会导致一个页不存在的异常。因此客户
操作系统的异常处理程序必须检查未完成的更新，如果有，那么它们就要被清空，然后导致异常的指令
要重新执行。

\subsubsection{物理内存}

最初的为每个域进行的内存分配，或者称为“预留”，是在域的创建时完成的，内存因此在域和域之间
静态地隔离开，保证了强的隔离性。最大的预留空间必须被声明：如果一个域内部的内存压力上升了，
它可以从 Xen 中声明出更多的内存分页，直到这个预留上限为止。相反，如果一个域想要节省资源，
比如说为了减少不必要的开销，它可以通过将分页释放回 Xen 的方式减少内存占用。

XenoLinux 实现了一个“气球驱动”，它通过在 Xen 和 XenoLinux 的页分配器之间来回传递分页
来调节域的内存占用。尽管我们可以直接修改 Linux 的内存管理例程，“气球驱动”使用现存的操作系统
的功能来实现内存的调节，因此减少了 Linux 移植的难度。尽管如此，半虚拟化可以被用来扩大内存驱动
的功能，举例来说，客户操作系统内存不足的处理机制可以被修改来自动通过从 Xen 要求更多的分页
来缓解内存压力。

大多数操作系统假设内存由至多几个大的相邻的分区组成。因为 Xen 并不保证分配内存相邻的区域，
所以客户操作系统通常自己创造一个连续物理内存的“幻觉”，尽管它们下层的硬件内存的分配是稀疏的。
从物理内存到硬件内存的映射是完全由客户操作系统来负责的，它可以直接维护一个由物理页帧号编号
的数组。Xen 通过提供一个由各个域共享可读的转换数组来支持高效的硬件到物理地址转换——这个数组
的更新必须经过 Xen 批准来确保操作系统拥有和自己相关的硬件页帧。

注意到尽管客户操作系统选择在大多数情况下无视硬件地址，它必须在访问页表的时候使用这个地址转换表，
因为这个过程必须使用硬件地址）。硬件地址也会被暴露给操作系统有限的几个内存管理系统来优化内存
访问。举例来说，一个客户操作系统分配特殊的硬件页以优化一个物理标号的缓存中的放置情况，或是使用
超页 (superpage) 映射自然对齐的连续硬件内存的分块。

\subsubsection{网络}

Xen 提供了虚拟防火墙-路由 (VFR) 的抽象，每个域有一个或者多个虚拟网络接口 (VIF) 逻辑地和
VFR 相连。VIF 看上去和现代的网络接口卡有相似之处：有两个缓冲区描述符的 I/O 环，一个用来传输
一个用来接收。每个方向还有一个关联的规则列表——如果模式匹配了，那么相应的动作会被执行。

Domain0 负责插入删除规则。在典型的状况里，规则是用来防止 IP 地址欺骗的，并且能确保基于目的
IP 地址和端口的正确的分离。规则还可以和 VFR 上的硬件接口关联起来。具体来讲，我们可以设置规则
进行实现传统的防火墙功能，比如说防止输入到非安全端口的网络连接。

要传输一个包客户操作系统只要向传输环中入队一个缓冲区描述符就可以了。Xen 会复制这个描述符，
而且为了保障安全，还要拷贝包头并且执行过滤规则。包的负载并不会被复制，因为我们使用分散-集中
(scatter-gather) DMA ，但是请注意，相关的页帧必须在传输完毕之后被绑定。为了保证公平性，
Xen 实现了一个简单地 Round-robin 包调度器。

为了有效的实现包接收，我们要求客户操作系统使用一个页帧来换取它收到的一个数据包，这就避免了 Xen
和客户操作系统之间的包的复制拷贝，尽管它需要页对齐的接收缓冲区在网络接口层被入队。当一个包
接收到，Xen 立即检查接收规则的集合，来决定目标的 VIF 并且在相关的接收环上将包缓冲区交换一个
页帧。如果没有可用帧，那么这个包就被舍弃掉。

\subsubsection{磁盘}

只有 Domain0 具有不用检查直接访问物理磁盘的能力。其它的域通过虚拟块设备 (VBD) 的抽象访问
非易失性存储设备，这个虚拟设备是由 Domain0 里运行的管理软件创建和配置的。允许 Domain0 管理
VBD 使得 Xen 内部的机制保持简单，而且避免了 Exokernel 使用的 UDF 方案那样的错综复杂的
解决方式。

一个 VBD 将关联的所有者和访问控制信息组织成了一个列表，它可以被 I/O 环机制访问。一个典型的客户
操作系统磁盘调度算法将请求在入队进环之前进行重排序以试图减少响应时间，并且应用区分的服务，
比如说，它可以选择以推测的预取请求作为代价来激进地调度同步的元数据请求。但是因为 Xen 对于实际的
磁盘布局有更深入的了解，我们也支持在 Xen 内部进行重排，这样回复就会被乱序返回。一个 VBD 设备
因此对于客户操作系统看上去有点像 SCSI 磁盘。

对于每个 VBD 虚拟机管理程序都维护一个转换表，它的条目是通过 Domain0 的特权控制接口来创建和
管理的。当接收到一个磁盘请求，Xen 检查 VBD 的标识符和偏置来生成相关的扇区地址和物理设备。
权限检查也在这一步进行。令拷贝的数据传输使用 DMA 在请求的域上的磁盘和绑定的内存页之间进行。

Xen 的服务使用 round-robin 的方式批量处理从相互竞争的域来的请求，在到达磁盘硬件之前还要再
经过一个标准的电梯调度器。当需要维护高级别的语义的时候域可以显式地向下传递重排序的壁垒，比如说
使用预写入的日志时。低级别的调度给予了非常好的吞吐率，请求的批处理保证了相对公平的访问。未来的
工作会考察提供更可预测的隔离和区分的服务，可能会使用现有的技术与调度器。

\subsection{创建新的域}

为了新的域创建初始的客户操作系统结构的任务基本上是靠使用特权控制接口访问新域的内存并通知 Xen
初始的寄存器状态的 Domain0 。这个方式相比于完全靠 Xen 本身创建新的域来说有几个优势，包括更少
的虚拟机管理程序的复杂性和更佳的可靠性（对特权接口的访问要有正确性检测，使我们在开发之初捕获了
很多 bug ）。

最重要的是对于移植一个全新的操作系统来说，这个过程是可扩展的。举例来说，开机时候的地址空间对于
Linux 内核的处理来说就要比 Windows XP 的更简单。对于所有的客户操作系统指明同一个固定的初始
内存布局是可行的，但是这会招致额外的操作系统内部的启动代码以将各种东西设置完成。不幸的是，这种
代码实现起来非常的有技巧性，为了简洁性与可靠性，在 Domain0 里实现能比在启动代码里实现更容易
诊断和调试。

\section{评估}

在这一章中我们展示对 Xen 的完整的性能测试。我们在一开始先将 Xen 和几种虚拟化技术做对比，然后
将把几个应用在原生的操作系统上并行地运行与将每个应用在自己的虚拟机里运行的系统吞吐量作对比。
我们之后还会评估 Xen 在客户操作系统之间提供的性能隔离，并且取得在同一个硬件上运行大量的操作
系统花费的总开销。在这些测量中我们使用的是 XenoLinux 的移植，因为这是我们最为成熟的客户操作
系统。我们希望 Windows XP 和 NetBSD 的移植有相似的性能，但是我们还没来得及进行完整的评估。

现有几个在同一台物理机上运行多个不同的 Linux 副本的已知的解决方案。VMWare 提供几个运行虚拟
x86 机器的商用产品，使用它们可以启动未经修改的 Linux 。最广为使用的版本是 VMWare Workstation ，
它包含一系列对宿主操作系统的特权内核扩展。Windows 和 Linux 宿主都可以被支持。VMWare 还提供
一个更高级的产品，叫做 ESX Server ，它将宿主操作系统以一个特殊的内核取代。通过这样做，它相较
Workstation 有更好的性能表现。ESX Server 还提供一个半虚拟化的网络接口，它可以在部属的环境
允许的情况下，通过在客户操作系统中安装一个特殊的设备驱动 (vmxnet) 来访问。

我们将 ESX Server 提交给了下面所属的测试套件，但是不能报告定量的结果，因为该产品的终端用户
协议不允许这样做。我们呈现的结果基于 VMWare Workstation 3.2 ，它在 Linux 宿主上面运行，
是没有测试发布限制的最新的 VMWare 产品。ESX Server 的性能表现大于等于 VMWare Workstation ，
受惠于它的原生的架构。尽管 Xen 必须使用修改的客户操作系统，它通过半虚拟化在性能方面显著超越了
ESX Server 。

我们还测量了用户态 Linux (UML) 的结果，它是一个越来越受欢迎的虚拟主机平台。UML 将 Linux
移植成 Linux 宿主的用户态进程。和 XenoLinux 类似，需要的修改都是限于平台相关的代码部分。
但是 UML 的代码与原生 x86 移植没什么共同点，因为运行环境相距甚远。尽管 UML 可以在未经修改的
Linux 宿主上运行，我们用的是“单一内核地址空间”的版本，通过对宿主操作系统打补丁来提升 UML 的性能。

我们也调查了另外三个在同一台 X86 机器上运行移植过的 Linux 版本的虚拟化技术。Connectix 的
Virtual PC 和后来的 Virtual Server 产品设计与 VMWare 相似都提供了全虚拟化。由于 Virtual
PC 的协议里有测试成绩的限制所以我们无法把其测试成绩公布出来。UMLinux 和 UML 的概念相仿，
只是代码不同，并且性能还不如 UML ，所以我们省略了测试结果。通过对宿主机操作系统进行修改来提升
UMLinux 的性能的工作正在进行。尽管 Plex86 本来是一个通用的 X86 虚拟机管理程序，但是它现在的
目标变成了只支持 Linux 客户操作系统。客户操作系统必须被特别地编译，以运行在 Plex86 上，但是
相比原生 X86 的代码层面改动很平凡。Plex86 的性能目前远远低于其它方案。

我们的实验都运行在一台 Dell 2650 双 Xeon 2.4GHz 处理器的服务器上，它具有 2GB 内存、
一块 Broadwell Tigon 3 千兆以太网卡还有一块日立 DK32EJ 146GB 万转 SCSI 硬盘。Linux
使用的均为 2.4.21 版本，在裸机或者 VMWare 上跑的时候编译到 i686 架构，在 Xen 上跑的时候
编译到 xeno-i686 架构，在 UML 上跑的时候编译到 um 架构。这台机器上的 Xeon 处理器支持 SMT ，
但是这项功能被关闭了因为这些内核目前都不具备支持 SMT 的调度器。我们保证每台客户机加上它们的
虚拟机管理程序的总内存与 Linux 裸机的内存数量相等。

RedHat 7.2 \footnote{时光飞逝，在正文部分我们的实验也是在 7.2 版本上进行的。
但是是 RedHat Enterprise Linux 7.2 的社区版本，和这篇译文中的 RedHat 7.2 完全是两个版本——译者注}
是我们始终使用的发行版，在 ext3 文件系统上安装。VM 被配置在 persistent raw mode 模式下
使用相同的磁盘分区，以达到最好的性能。使用相同的文件系统镜像还消除了磁盘寻道时间和传输速率的
潜在的差别。

\subsection{相对性能}

为了测试不同种类的虚拟化技术相对于物理机的开销我们进行了一系列实验。复杂的、使用整个系统的应用
层的基准测试被用来描述系统在一系列服务器类型的负载下的性能表现。既然 Xen 和 VMWare 任一款产品
当前都还不支持多处理器的客户操作系统，测试机被配置只使用一个处理器，我们之后再测试并行的客户
操作系统的性能。测试结果是多次测试取得的中位数。

图 3 中的第一族条形代表了一个对虚拟机管理程序相对“轻松”的场景。SPEC CPU 测试集包含了一系列
长时间运行的计算密集型应用以测量系统处理器、内存和编译器质量的性能表现。这个测试集进行的 I/O
操作很少，与 OS 也没有什么互动。三种虚拟机管理程序的开销都比较小，因为几乎所有的 CPU 时间都
花费在执行用户态的代码。

下一族条形代表了在 ext3 文件系统和 gcc 2.96 编译器上编译 Linux 2.4.21 内核的默认设置总共
花费的时间。原生 Linux 消耗了大约 7\% 的 CPU 时间在操作系统里，主要是进行文件 I/O 、调度和
内存管理。在虚拟机管理程序的例子里，系统时间大小不一地延长了，虽然 Xen 的额外开销只有 3\% ，
其它虚拟机管理程序的运行减缓更加明显。

两个实验是用 PostgreSQL 7.1.3 数据库进行的，由 Open Source Database Benchmark 跑在
默认设置下。我们呈现了多用户信息检索 (IR) 和在线传输处理 (OLTP) 两种负载的测试结果，都是按
“元组每秒”的单位测量的。为了获得正确的结果需要对测试套件进行少量修改，因为 UML 有一个在高负载
下丢失虚拟时钟中断的 bug 。测试通过一个 Unix domain socket 通过 PostgreSQL 原生的 API
驱动数据库。PostgreSQL 给操作系统施加了显著的负载压力，这是通过 VMWare 和 UML 表现出的
显著的虚拟化开销反映出来的。尤其是 OLTP 测试需要很多同步的磁盘操作，导致了很多保护域的迁移。

Dbench 程序是一个从工业标准的 NetBench 派生出来的文件系统基准测试。它模拟了由 Windows 95
客户端施加给文件服务器上的压力。在此我们检验了一个客户端进行 90000 次文件系统操作的吞吐率。

SPEC WEB99 是一个复杂的应用层基准测试，用来测试 web 服务器和托管它们的系统。负载是页面请求的
复杂的组合：30\% 需要动态内容生成、16\% 是 HTTP POST 操作、0.5\% 执行 CGI 脚本。在服务器
运行过程中它会生成访问和 POST 请求的日志，所以磁盘负载并不是单纯只读的。测量因此反映了综合的
OS 性能，除了 web 服务器本身的性能之外还包括文件系统和网络的性能。

多个客户端机器被用来为服务器的测试提供负载，每个客户端模拟了一组用户并行地访问网站。
基准测试用不同数量的模拟用户来进行重复测试，以决定最大的可被支持的用户数量。SPEC WEB99
定义了服务质量的最小值，以模拟用户必须接收来构成页面的请求：在一系列请求中用户必须接收累计
320 Kb/s 的带宽。允许有一个预热的阶段，在此，同时的客户端数量缓慢增加，以使得服务器能预先
加载它们的缓存。

对于我们的测试环境，我们使用 Apache HTTP server 1.3.27 版本，安装 modspecweb99 插件来
运行大部分而不是全部的动态内容生成—— SPEC 的规范要求 0.5\% 的请求使用 CGI ，fork 出一个
新的进程。更好地绝对的性能结果可以通过 TUX 的帮助达成，TUX 是一个 Linux 内核的静态 web
服务器，但是我们没有选择这个作为负载，因为我们感觉它不太可能代表了我们的真实世界应用场景。
另外，尽管 Xen 的性能在使用 TUX 的时候有所提高，VMWare 的性能受到了大幅度影响，因为在执行
客户操作系统的内核的时候消耗了大量的时间模拟 Ring 0 。

SPEC WEB99 测试整个系统。在测量阶段有高达 180Mb/s 的 TCP 网络流量以及 2GB 的测试数据集
带来的相当活跃的磁盘读写。基准测试是 CPU 相关的，一大部分时间用在了客户操作系统内核、
处理网络栈、文件系统操作以及在多个用来处理负载的 Apache httpd 进程之间调度里。XenoLinux
表现很好，开销在原生 Linux 的 1\% 以下。VMWare 和 UML 的性能都不怎么好，可以支持的用户数量
只是原生 Linux 的不到 1/3 。

\subsection{操作系统基准测试}

为了更加准确地测量 Xen 和其它虚拟机管理程序的开销的部分，我们针对特定的子系统进行了一系列小的
实验。我们通过 McVoy 的 lmbench 项目测试了虚拟化带来的开销。我们使用的是 3.0-a3 版本，
因为它解决了很多关于 Seltzer 的 hbench 工具的保真度的问题。Lmbench 的 OS 性能测试子集
包含了 37 个微测试程序。在原生 Linux 的情况下，我们既提供了单处理器 (L-UP) 的图表，也提供了
SMP 内核 (L-SMP) 的图表，因为我们对 SMP 系统在很多情况下的额外的锁带来的性能开销感到非常惊讶。

在 37 个微测试程序中的 24 个里，XenoLinux 的性能表现都和原生 Linux 接近，和单处理器的内核
非常接近，比 SMP 内核要好。在表 3 ~ 5 中，我们展示了测试系统中有意思的性能差异，Xen 特别明显
的性能缺陷使用粗体显示出来。

在进程微测试中（表 3），Xen 展示了相比原生 Linux 更慢的 fork 、 exec 和 sh 的性能表现。
这是在预料之内的，因为这些操作需要大量的页表更新操作，它们必须被 Xen 批准。但是，半虚拟化的
方法使 XenoLinux 可以批量处理更新请求。创建新页表展示了一种理想情况：因为不用将更新请求更
早提交，XenoLinux 可以在 2048 次更新时再分期提交一个 hypercall （也就是达到了缓冲区的
容量上限）。因此，每个更新的 hypercall 的地址空间容量是 8MB 。

表 4 展示了不同工作集大小的情况下不同数量的进程进行上下文切换的时间开销。Xen 有 1 微秒到 3
微秒的开销，因为它执行一个 hypercall 来改换页表的基地址。但是，更大的工作集大小的上下文切换
的结果表现了相比于缓存作用来说，这个开销是相对较小的。非同寻常的是，VMWare Workstation
在这些基准测试中的性能还不如 UML ，而这恰好又是 ESX Server 中的改进应该能减小开销的地方。

在表 5 中展示的 mmap 延迟和缺页延迟是很有意思的，因为每个页需要陷入 Xen 两次——一次接收硬件
异常并将细节传给客户操作系统，另一次代替客户操作系统来更新页表项。尽管如此，开销相对而言是
可以接受的。

表 3 中一个微小的异常现象是 XenoLinux 的信号处理延迟比原生 Linux 还要小。这个基准测试
不需要任何对 Xen 的调用，我们猜想 0.75 毫秒 (30\%) 的加速是因为 XenoLinux 里一个偶发的
缓存对齐 (cache alignment) ，因此我们认为基准测试的成绩不能过分当真了。

\subsubsection{网络性能表现}

为了评测虚拟网络的开销，我们测试了千兆以太网上的 TCP 性能。在所有测试中，我们使用了一台配置类似
的 SMP 机器运行原生 Linux 充当一个终端。这使得我们可以分开测量接收和传输的性能表现。发送和
接收的 socket 缓冲区都设置为 128KB 的大小，因为我们发现这时候性能最好。测试结果是 9 组
分别传输 400MB 的实验的中位数。

表 6 展示了两组测试结果，一组使用默认的以太网 MTU 也就是 1500 bytes ，另一组使用 500 byte
的 MTU （这个值被拨号的 PPP 客户端广泛使用）。结果显示 XenoLinux 的虚拟网络驱动使用的
page-flipping 技术避免了数据拷贝的开销，因此取得了非常低的每字节开销。当 MTU 值是
500 bytes 的时候，每个包的开销就是主导因素了。传输的防火墙和接收的分离都对吞吐率有不利影响，
但是只有 14\% 。

VMWare 模拟了一块 pcnet32 网卡来和客户虚拟机进行通讯，它提供了一个相对清晰的基于 DMA 的
接口。ESX Server 还为兼容的客户操作系统提供了一个特殊的 vmxnet 驱动，它提供了明显的
网络性能的改进。

\subsection{并行的虚拟机}

在这一小节中，我们将在客户操作系统中运行多个应用程序的性能和直接在裸硬件上运行它们的性能做比较。
我们的重点是使用 Xen 得到的那些结果，但是当恰当的时候我们我们也会评价其它虚拟机管理程序的性能。

表 4 展示了在一台两 CPU 的机器上并行运行 1、2、4、8 和 16 份的 SPEC WEB99 基准测试程序的
结果。原生 Linux 配置使用了 SMP ，在其上我们并行运行多份 Apache 。在 Xen 的情况下，
每个 SPEC WEB99 的测试实例被运行在自己的单处理器 Linux 客户操作系统中（当然里面还有 sshd
等管理用的进程），每个 web 服务器使用了不同的 TCP 端口号以保证每份程序都可以并行执行。注意到
运行 C 个同时的连接需要的 SPEC 数据集大小是 (25 + (C * 0.66)) * 4.88 MBytes ，对于
1000 个连接大致需要 3.3GB 。这个已经足够大，可以彻底调用磁盘和缓存子系统。

达到高 SPEC WEB99 成绩既需要高的吞吐率也需要有限的延迟：举例来说，如果客户端请求因为严重拖延
的磁盘读取而受到阻碍，那么这个连接就会被归类成不符要求的，从而不会对成绩有贡献。因此，虚拟机
管理程序及时地调度域就是非常重要的。默认情况下，Xen 使用 5ms 的时间片。

在一个 Apache 的实例的情况下，增加一块 CPU 使得原生 Linux 以 28\% 的幅度提升 4.1 小节的
成绩，达到 662 个符合要求的客户端。然而，最好的叠加吞吐率是在运行两个 Apache 实例的时候达到
的，暗示了 Apache 1.3.27 可能有一些 SMP 支持上的问题。

当只运行一个域的时候，Xen 因为缺乏客户操作系统的 SMP 支持而受到妨碍。但是 Xen 的中断负载
均衡器能识别出空闲的 CPU 从而将所有的中断处理分配给它，从而在单 CPU 的基础上提升 9\% 的成绩。
当域的数量提升，Xen 的性能就只和在原生 Linux 相差百分之个位数了。

之后我们进行了使用 OSDB 套件测试运行的多个 PostgreSQL 实例的一系列实验。在同一台 Linux
机器上运行多个 PostgreSQL 实例被证明很困难，因为通常是使用一个 PostgreSQL 实例运行多个
数据库。但是这么做就不能使不同用户使用独自的数据库设置。我们使用了 chroot 和软件补丁的组合
来避免在不同的 PostgreSQL 实例之间的 SysV IPC 的命名空间的冲突。作为对比，Xen 允许每个实例
在自己的域中运行从而实现了易用的配置。

在图 5 中我们展示了 Xen 在运行 1、2、4、8 个 OSDB-IR 和 OSDB-OLTP 实例时取得的叠加吞吐率。
当加入了第二个域，对于第二块 CPU 的完整的使用几乎使得总吞吐率翻番了。继续增加域的数量导致叠加
吞吐率的一定程度的下降，这是由于上下文切换和磁盘磁头移动导致的。在一个 Linux 操作系统上运行多个
PostgreSQL 的叠加的成绩比在 Xen 上运行要低 25~35\% 。这个的原因并不是很清楚，但是似乎
Postgre SQL 有一些 SMP 的问题，而且对于 Linux 的块缓存 (block cache) 利用不佳。

图 5 还展示了 8 个域之间的性能分化。Xen 的调度器被配置以给每个域赋一个在 1 到 8 之间的整数
权值。图上的每个条形代表了每个域的吞吐率成绩。在 IR 基准测试上，权值对于吞吐率有精确的影响，
每部分的大小相对预期值在 4\% 以内。但是在 OLTP 的情况下，占有资源更多的域并没有取得更好的成绩：
高水平的同步磁盘活动显示我们现在的磁盘调度算法中的缺陷，这个缺陷使得这些占有资源更多的域没有
达到理想成绩。

\subsection{性能隔离}

为了展示 Xen 提供的性能隔离，我们希望在 Xen 和其它基于操作系统的性能隔离技术比如资源容器之间
进行一次测试 (bakeoff) 。然而在目前似乎并没有基于 Linux 2.4 的实现可供下载。QLinux 2.4
尚未发布，而且它是针对多媒体应用程序进行 QoS 而不是在服务器环境下提供完全的防御型的隔离。
Ensim 的基于 Linux 的 Private Virtual Server 产品似乎是最完整的实现，据称可以实现对
CPU 、磁盘、网络和物理内存资源的控制。我们正在和 Ensim 进行交涉，希望能在以后发布比较的
测试结果。

尽管缺少对照的比较，我们展示的结果显示 Xen 的性能隔离和预期相符，即使在恶意的负载状况下也是
成立的。我们运行了四个使用相同的资源分配的域，其中两个运行之前已经测量过的负载 (PostgreSQL
/ OSDB-IR / SPEC WEB99) ，另外两个域运行一对极为恶意的进程。其中一个（第三个域）并行地
运行持续的 dd 消耗磁盘带宽，而且运行文件系统操作密集的负载，在庞大的目录中创建大量的小文件；
另一个（第四个域）运行一个 fork 炸弹，同时运行一个虚拟内存密集的应用，它尝试分配 3GB 的
虚拟内存，如果失败就释放全部页面，然后重启。

我们发现 OSDB-IR 和 SPEC WEB99 的测试成绩只被两个运行干扰程序的域轻微地影响了——只比之前的
测试分别低了 4\% 和 2\% 。我们把这个归因于额外的上下文切换和缓存效应带来的开销。我们认为在
现在我们使用的较为简单地磁盘调度策略上这个测试结果略有侥幸，但是在现在的策略上，它还是显示出来
可以提供对于页交换和磁盘活动密集型的域的足够的隔离，以使得基准测试能正常运行。
VMWare Workstation 达到了类似级别的隔离，但是它的绝对性能比较低。

我们在原生 Linux 上重复了类似的实验。果不其然，干扰程序将整机变得对于两种基准测试程序完全
不可使用，导致 CPU 时间在操作系统内被消耗殆尽。

\subsection{可扩展性}

在这一小节中，我们检验了 Xen 扩展到 100 个域的能力。我们讨论运行多个客户操作系统和相关的
应用程序的实例的内存需求，并且测量了 CPU 性能在它们运行时的开销。

我们测量了一个运行 XenoLinux 和 RH 7.2 默认的软件集（包括 sshd 和 Apache web server）
的域的最小的物理内存需求。这个域在启动时预留了 64MB 内存空间，并且限制了最大可以增长到的值。
客户操作系统通过尽可能将所有页面返回给 Xen 来最小化它的内存占用。没有配置任何交换空间，这个域
可以将它的内存占用减小到 6.2MB ；允许使用交换设备将其进一步减小到 4.2MB 。一个静态的域可以
保持在这个最小化的状态直到一个 HTTP 请求或者暂时的服务使得机器需要更多的内存。在这种情况下，
客户操作系统会向 Xen 讨回页面，其内存占用可以达到它设置的最大值。

这显示了内存使用的开销不太可能在在一台现代的服务器级别的计算机上运行 100 个域的情况下成为问题
——应用程序的数据和缓存需要的内存远远多于操作系统或者应用程序的代码本身占据的内存。Xen 自己在
一个域里面只会占用 20KB 的固定空间，不像别的虚拟机管理程序还要维护影子页表等等。

最后，我们检测了在很多个域而不是进程之间的上下文切换带来的开销。图 6 展示了在双 CPU 服务器上的
1 到 128 个域运行 SPEC CINT2000 测试集的子集获得的正规化的叠加吞吐率。代表原生 Linux 的线
几乎是平的，意味着在这个测试中在进程之间调度不会带来叠机的性能的损失；Linux 将它们都归类为
计算密集的，并且以超过 50ms 的长时间片进行调度。作为对比的是较低的那根线代表了 Xen 在配置使用
它默认的 5ms 的调度时间片最大值时候的吞吐率。尽管同时在一台服务器上运行 128 个并行的计算密集
型进程对于我们的应用场景不是很可能，但是 Xen 处理的还是很好的：运行 128 个域只比原生 Linux
损失了 7.5\% 的总吞吐率。

在极端负载条件下，我们测量了对于其中一个运行 SPEC CINT2000 子集的域的用户对用户
(user-to-user) UDP 延迟。我们测量的平均响应时间是 147ms （标准差 97ms）。我们又对第 129
个如果在不测试情况下是空闲的域进行同样的实验，记录到的平均响应时间是 5.4ms （标准差 16ms）。
这些数据是非常鼓舞人心的，因为尽管背景负载是很高的，但是交互的域还是响应迅速。

为了了解 7.5\% 的性能开销的原因，我们将 Xen 的调度时间片设置为 50ms，也就是 ESX Server
使用的默认值）。结果得到了和原生 Linux 非常接近的一条吞吐率曲线，性能差异几乎消失了。
尽管如此，和预期的一样，在高负载条件下的交互性能被这个设置负面地影响了。

\section{相关工作}

虚拟化已经被应用在商业和科研的操作系统上将近 30 年了。IBM VM/370 首次使用虚拟化技术来提供
遗留代码的二进制支持。VMWare 和 Connectix 都将日常的 PC 硬件进行了虚拟化，使得多个操作系统
能在同一个主机上运行。这些例子都实现了下层硬件（至少是硬件的一个子集）的全虚拟化而不是半虚拟化
对操作系统提供一个修改过的接口。正如在我们的评估中展示的，全虚拟化尽管能够更容易地支持现成的
操作系统，但是它的设计对于性能是不利的。

Disco 使用的虚拟机管理程序方法来使常见的操作系统可以在 ccNUMA 的机器上高效率地运行。为了在
MIPS 架构上虚拟地执行，客户操作系统必须作出少量的修改。另外，还有一些修改是为了提高性能。

在目前，我们发现两个其它的系统也使用了半虚拟化的方式：IBM 当前对于它们的 Z 系列大型机支持
一个半虚拟化的 Linux 的版本，使得大量的 Linux 实例可以并行运行。在前文讨论过的 Denali 是
一个现代的隔离内核，试图提供一个可以承载大量的虚拟操作系统实例的系统。

除了 Denali 之外，我们还意识到存在其它两个使用低级虚拟化来构建分布式基础设施的尝试。vMatrix
项目基于 VMWare 来构建一个在不同的机器间移植代码的平台。因为 vMatrix 是构建与 VMWare 之上
的，所以它更关心的是更高级的分布式的问题而不是虚拟化本身。另外 IBM 提供一个 managed hosting
服务，虚拟的 Linux 实例可以从 IBM 大型机上租借。

PlanetLab 项目构建了一个目标是作为地理分布式网络服务的研究和开发平台的基础设施。这个平台的
目标群体是研究人员，试图把物理计算机分割成 silver ，来为用户提供低级的访问。当前的实现使用
VServers 和 SILK 来管理操作系统内部的资源共享。

我们与操作系统扩展和活跃网络的社区具有相同的动机。但是运行 Xen 的时候没有必要为了“安全的”代码
或是一定保证的终止而进行检查——在两种情况下只有客户机才会受到影响。因此，Xen 提供了一个更加通用
的解决方案：承载的上层代码没有经过可信的编译器数字签名的必要、没有伴随一个安全性证明的必要、
没有使用特殊的编程语言编写的必要、也没有依赖特殊的中间件的必要。上述的那些技术当然可以继续在
Xen 上面运行的客户操作系统里使用。尤其在临时的、开启一个新的域的开销很难弥补的负载状况下，
上述方法尤其有用。

对于语言层面的虚拟机方法，类似的结论也成立：尽管可以管理资源的 JVM 理所应当可以承载不可信的
应用程序，这些应用还是必须得被编译成 Java 字节码并且遵循系统的特殊安全模型的。另外，Xen
直接可以支持语言层的虚拟机在客户操作系统里作为应用运行。

\section{讨论与结论}

我们在上文介绍了将计算机划分成多个不同的运行客户操作系统的域的虚拟机管理程序 Xen 。
我们的半虚拟化设计尤为强调性能与资源管理。我们还描述和评估了 XenoLinux ，一个 Linux 2.4
内核的全功能的、可在 Xen 上运行的移植。

\subsection{后续工作}

我们相信 Xen 和 XenoLinux 都足够完整被更多的用户接受，所以在不远的将来打算发布一个公开的版本。
一个测试版目前正经由几个特挑的机构评估；这个阶段完成后，一个 1.0 版本会在我们主页上发布。

在第一个版本发布以后我们计划了几个对 Xen 的扩展和改进。为增加虚拟块设备的效率，我们计划实现一个
在块目录里编号的共享的全局缓冲区缓存 (universal buffer cache)。这个可以在不牺牲隔离性的
前提下为我们的设计增加可控的数据共享机制。为虚拟块设备增加写时复制的语义可以允许它们被安全地
在域与域之间共享，同时仍然保证文件系统地隔离。

为了提供更好的物理内存性能我们计划实现一个 LPC (last-chance page cache) ，也就是一个系统
范围内的空页的列表，当且仅当机器的内存没有被占用满它的长度才不是 0 。LPC 在客户操作系统的虚拟
内存选择回收一个干净页的时候会被使用，它会被添加进空页列表的尾部而不是完全被抛弃，因此，一个在
它被 Xen 重新分配之前产生的页异常不用任何磁盘请求就可以被满足。

Xen 一个重要的角色就是充当 XenoServer 项目的基石，这个项目看得比单个机器更为长远，它计划
建造可以支持互联网际的计算设施的控制系统。我们的设计的关键是资源的使用被精确记录并由那个任务
的承担者付账——假如付款用的是现金我们可以使用“高峰期收费”策略来缓解过多的需求，使用超额受益来
支付额外的机器。这就需要准确和及时的 I/O 调度和对于恶意负载的更高的抵抗力。我们还计划将计费
整合进我们的块存储架构来创造虚拟块设备的租借。

为了提供更好的对于管理的支持，我们正整合更好的日志系统。我们还在开发额外的 VFR 规则，我们希望
这些规则能够检测和预防反社会的网络行为。最后，我们在继续我们对于 XenoXP 的开发，着重编写网络
和块设备的驱动，试图完整地支持 IIS 这样的企业级服务。

\subsection{结论}

Xen 为部署一大批网络中心的服务提供了有力的平台，例如本地对网络内容的动态镜像、媒体流的转码和
分发、多人游戏和虚拟现实服务、为短暂连接的设备提供一个更长久一些的网络存在的“聪明代理”。

Xen 直指部署这些服务的最大的困难——目前不能在低实例化的代价的情况下承载短时间的临时服务。通过
允许 100 个操作系统同时在一个服务器主机上运行，我们从两个维度上减小了这个开销。另外通过将设置
和配置操作系统变成纯粹的软件的考量，我们将托管服务的时间粒度显著减小了。

正如我们在第四章展示的实验结果，Xen 上运行的 XenoLinux 的性能在实际上和 Linux 系统是基本
相同的。这个事实是通过两个组件之间 (Xen 和 XenoLinux) 接口仔细的设计得来的，意味着使资源
管理设施可用没有什么明显的开销。我们接下来对 BSD 和 Windows XP 的移植会证明 Xen 的接口
是可以通用的。
