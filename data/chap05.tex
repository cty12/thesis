\chapter{系统的测试与评价}

\section{不同虚拟化方案对负载性能影响的比较}

在本节，首先介绍 Tsinghua NOVA 项目中的基准测试套件 Omegabench ，然后通过在不同的
虚拟化平台上运行同样的模仿真实环境的负载，对不同虚拟化方案的效果和实用性作出评估。

\subsection{实验环境}

实验使用的硬件是一台清华同方“超强”系列的塔式服务器，型号是 TP-240 。具体配置如下：

\begin{lstlisting}
Tsinghua-Tongfang TP-240 tower server
    |-- Dual Intel Xeon E5606 @ 2.13 GHz
    |-- 32 GB DDR3 ECC memory
    |-- WDC WD10EALX-229
\end{lstlisting}

宿主机使用的操作系统是 CentOS 7.2 ，也就是目前最新的版本。相应的软件栈也在实验之前更新
到 Red Hat EPEL 软件仓库中的最新版本。

\subsection{负载类型的选择}

常见的基准测试软件可以按照负载类型分成两类。一种是运行理想化的负载，例如计算圆周率的
SuperPI 。在实际应用中，恐怕没有用户会每天使用计算机无穷无尽地计算圆周率，所以这种
负载只能说是对真实世界的计算问题的一种简单的抽象。另一种是运行现实中真实存在的综合性
的计算需求，比如 SPEC CPU 基准测试，包括了许多科学计算任务——流体力学计算、声音识别
等等，又比如 GeekBench ，套件中包含了图象压缩、数据加密、哈希计算等多种个人用户常用的
计算功能的性能评估。

本测试套件 Omegabench 主要是面向大型服务器集群，尤其是虚拟化集群。在本测试套件中，
主要包含了以下几种类型的负载：

\begin{enumerate}
    \item web 服务器：主要包括传统的 HTTP 服务器 Apache httpd 和轻量级
    的 HTTP 服务 mongoose 两个测试项目。基准测试使用的客户端是 Apache 自带的测试程序，
    适用于几乎所有的 HTTP 服务。主要测试指标是服务器的相应速度和吞吐量；
    \item 存储和数据库服务：这个类别包括三个功能互不相同的服务。一是 memcached
    ，一个通用的分布式数据内存缓存 (memory caching) 服务，二是 redis ，一个内存数据存储
    服务，既可以作为缓存 (caching) 也可以作为一个内存数据库，三是 mysql ，一个著名的
    数据库服务；
    \item 压缩和加密：这个类别目前只包含一个测试项目，也就是 xz 的解压缩以及
    tar 解包的综合测试。这个测试既考验 CPU 性能也考验磁盘 I/O 性能。
\end{enumerate}

\subsection{资源限制}

在这一小节，我们说明本次实验对于 LXC 与 KVM 两种虚拟化平台的配置是公平的。

\subsubsection{通过 cgroup 对 LXC 做资源限制}

在 LXC 虚拟容器中，我们通过修改容器的配置文件来对其进行内存的配额限制。在
~\ref{subsubsec:cgroup}小节提到过，现代的 Linux 内核可以通过 cgroup 进行进程组的资源
调配，所以在这里，我们就要修改 cgroup 的相关参数。

以名为 test-fedora-1 的容器举例，修改配置文件，添加：

\begin{lstlisting}
# limit memory
lxc.cgroup.memory.limit_in_bytes = 8G
lxc.cgroup.memory.memsw.limit_in_bytes = 10G
\end{lstlisting}

就可以将总内存限制为 8GB ，内存加 swap 限制为 10GB 。配置文件修改后的 LXC 容器启动日志
如下\footnote{日志较长，这里只提取了关键信息}：

\begin{lstlisting}
cgfs.c:do_setup_cgroup_limits:1922 - cgroup 'devices.allow' set to 'c 5:2 rwm'
cgfs.c:do_setup_cgroup_limits:1922 - cgroup 'memory.limit_in_bytes' set to '8G'
cgfs.c:do_setup_cgroup_limits:1922 - cgroup 'memory.memsw.limit_in_bytes' set to '10G'
cgfs.c:do_setup_cgroup_limits:1926 - cgroup has been setup
\end{lstlisting}

也可以从文件中读取。注意因为 /proc/meminfo 没有对 cgroup 进行过优化，所以它读出来的
内存信息是宿主机的总内存容量而不是容器内部限制使用的配额：

\begin{lstlisting}
[root@test-fedora-1 ~]# cat /sys/fs/cgroup/memory/lxc/test-fedora-1/memory.limit_in_bytes
8589934592
[root@test-fedora-1 ~]# cat /sys/fs/cgroup/memory/lxc/test-fedora-1/memory.memsw.limit_in_bytes
10737418240
\end{lstlisting}

可见，此时该虚拟容器 test-fedora-1 的内存资源已得到了限制。

使用类似的方法可以对 CPU 数量等关键参数进行设置。在本次实验中，因为 LXC 与 KVM 的性能测试是
先后而不是同时运行，所以二者的设置都是使用宿主机全部的 CPU 个数。

\subsubsection{对 QEMU-KVM 进行资源限制}

KVM 有数量众多的前端工具，比如命令行的 virsh (Virtual Shell) 和图形界面的 virt-manager
。我们使用 Red Hat 自家的 GNOME Boxes 作为前端来配置 KVM 的客户虚拟机，这也是 RHEL/CentOS
的默认方案。

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\textwidth]{gnome-boxes-config}
    \caption{通过 GNOME Boxes 配置客户虚拟机}
    \label{fig:gnome-boxes-config}
\end{figure}

如图~\ref{fig:gnome-boxes-config}所示，通过图形界面能简单地配置内存和处理器的配额。
通过 virsh dominfo 命令可以进行查看：

\begin{lstlisting}
$ virsh dominfo fedora22-wor
Id:             3
Name:           fedora22-wor
UUID:           3fe3e047-aa2e-4280-90d4-97c0816e3529
OS Type:        hvm
State:          running
CPU(s):         8
CPU time:       305743.4s
Max memory:     8373248 KiB
Used memory:    8373184 KiB
Persistent:     yes
Autostart:      disable
Managed save:   no
Security model: selinux
Security DOI:   0
Security label: unconfined_u:unconfined_r:svirt_t:s0:c355,c485 (enforcing)
\end{lstlisting}

\subsection{运行本测试套件}

我们的测试套件的远端仓库 (repository) 托管在 GitHub 上，使用 git 进行版本控制。可以
以源代码的形式获得这个套件。

\subsubsection{目录结构}

工程的根目录包含一个叫 tests/ 的目录，和其它若干个以测试用例的名称命名的目录，比如 mongoose/
和 redis/ 等。tests/ 目录包含了全部测试脚本，和测试使用的配置文件，还有以表格或者图表的形式
收集测试结果的相关代码。而以测试项目命名的目录中一般有一个用于自动下载和编译安装测试用的服务
的脚本，还有一个开启服务器的脚本。通过先后运行这两个脚本可以完成测试用服务的自动部署和配置。
还有一个文件保存编译的时候使用的配置信息，比如服务器软件的版本、所需依赖的版本等。

\subsubsection{安装依赖和编译测试服务}

以 Apache httpd 为例，首先通过保管理器安装相关的依赖：

\begin{lstlisting}[language=bash]
dnf install tar wget # required when using Fedora mininal installation
dnf install gcc make
dnf install pcre-devel
dnf install libxml2-devel
dnf install xz       # for testing xz and tar
\end{lstlisting}

然后需要在相应的 shell 配置文件比如 .bashrc 中定义 \$OMEGA\_HOME 这个变量：

\begin{lstlisting}[language=bash]
export OMEGA_HOME="your/preferred/dir/here"
\end{lstlisting}

然后运行叫做 \$OMEGA\_HOME/apache-httpd/download-compile-apache.sh 即可自动完成
httpd 以及 PHP 解释器的编译安装，而且会自动生成一个测试用的配置文件 httpd.conf 。使用
apachectl 可以控制 httpd 的运行与停止，通过 -f 参数可以指定配置文件。

\subsubsection{运行测试程序}

仍然以 Apache httpd 举例，通过运行 run-ab-tests.py 可以自动进行基准测试并以列表和
柱状图的形式显示出性能数据。

相应的测试程序配置文件是 ab-tests-config.json ，一个简单的测试本机的例子如下：

\begin{lstlisting}
{
  "ab": "$OMEGA_HOME/apache-httpd/install/bin/ab",
  "num_req": 128,
  "concurrency": 8,
  "port": 7000,
  "ip_addr": [
    "127.0.0.1"
  ]
}
\end{lstlisting}

在这个例子中，测试客户端一共向服务器发送 128 个 HTTP 请求 (requests) ，
并发度 (concurreny level) 是 8 ，连接到服务器的 7000 端口，服务器的 IP 地址是
127.0.0.1 。

\subsection{实验结果}

\subsubsection{Web 服务器}

Mongoose 和 Apache httpd 的测试结果分别如表~\ref{tab:mongoose-perf}
以及~\ref{tab:apache-perf}。

\begin{table}[H]
    \centering
    \caption{Mongoose 的性能测试结果}
    \begin{tabular}{||c c c c||}
        \hline
        FIELD & KVM & LXC & BAREMETAL \\
        \hline
        \hline
        Time per request (ms) & 255.284 & 260.869 & 276.233 \\
        \hline
        Requests per sec & 31.34 & 30.67 & 28.96 \\
        \hline
        Transfer rate (KB/s) & 3.4 & 3.32 & 3.14 \\
        \hline
        Time taken (s) & 4.085 & 4.174 & 4.42 \\
        \hline
    \end{tabular}
    \label{tab:mongoose-perf}
\end{table}

\begin{table}[H]
    \centering
    \caption{Apache httpd 的性能测试结果}
    \begin{tabular}{||c c c c||}
        \hline
        FIELD & KVM & LXC & BAREMETAL \\
        \hline
        \hline
        Time per request (ms) & 250.991 & 245.327 & 265.69 \\
        \hline
        Requests per sec & 31.87 & 32.61 & 30.11 \\
        \hline
        Transfer rate (KB/s) & 7.07 & 7.23 & 6.67 \\
        \hline
        Time taken (s) & 4.016 & 3.925 & 4.251 \\
        \hline
    \end{tabular}
    \label{tab:apache-perf}
\end{table}

Mongoose 中虽然 LXC 以微弱劣势不敌 KVM ，但两者的差距在 $2\%$ 左右，可以近乎认为性能
表现相同；在 Apache httpd 的测试中，LXC 又反而以微弱的优势反超。在此项测试中，可以近似地
认为二者表现相同。

\subsubsection{存储和数据库服务}

Redis 、Memcached 还有 Mysql 的测试情况分别如表~\ref{tab:redis-perf}、
~\ref{tab:memcached-perf}还有~\ref{tab:mysql-perf}所示：

\begin{table}[H]
    \centering
    \caption{Redis 的性能测试结果}
    \begin{tabular}{||c c c c||}
        \hline
        FIELD (req/s) & KVM & LXC & BAREMETAL \\
        \hline
        \hline
        PING\_INLINE & 29325.51 & 46382.19 & 53475.94 \\
        \hline
        PING\_BULK & 28352.71 & 48053.82 & 47892.72 \\
        \hline
        SET & 25614.75 & 47460.84 & 56053.81 \\
        \hline
        GET & 24137.10 & 46882.33 & 51387.46 \\
        \hline
        INCR & 30854.68 & 46838.41 & 50175.61 \\
        \hline
        LPUSH & 24384.30 & 54377.38 & 52548.61 \\
        \hline
        RPUSH & 21786.49 & 47080.98 & 59136.61 \\
        \hline
        LPOP & 24289.53 & 46620.05 & 53333.33 \\
        \hline
        RPOP & 25062.66 & 46882.33 & 57142.86 \\
        \hline
        SADD & 31104.20 & 47415.84 & 57438.25 \\
        \hline
        SPOP & 27570.99 & 47014.57 & 58207.21 \\
        \hline
        LRANGE\_100 & 31466.33 & 27979.86 & 29291.15 \\
        \hline
        LRANGE\_300 & 12802.46 & 11087.70 & 12597.63 \\
        \hline
        LRANGE\_500 & 9324.01 & 8588.11 & 9441.09 \\
        \hline
        LRANGE\_600 & 7763.37 & 7290.76 & 7421.15 \\
        \hline
        MSET (10 keys) & 20846.36 & 44306.60 & 57372.34 \\
        \hline
    \end{tabular}
    \label{tab:redis-perf}
\end{table}

\begin{table}[H]
    \centering
    \caption{Memcached 的性能测试结果}
    \begin{tabular}{||c c c c||}
        \hline
        FIELD & KVM & LXC & BAREMETAL \\
        \hline
        \hline
        Total time (s) & 5.315 & 1.714 & 1.134 \\
        \hline
    \end{tabular}
    \label{tab:memcached-perf}
\end{table}

\begin{table}[H]
    \centering
    \caption{Mysql 的性能测试结果}
    \begin{tabular}{||c c c c||}
        \hline
        FIELD & KVM & LXC & BAREMETAL \\
        \hline
        \hline
        Total time (s) & 3.8928 & 4.3085 & 3.856 \\
        \hline
        Response time (ms) & 302.95 & 334.7 & 303.03 \\
        \hline
    \end{tabular}
    \label{tab:mysql-perf}
\end{table}

Memcached 中 LXC 的表现非常优秀，相较 KVM 节省了 $67.8\%$ 的总时间，以至于接近裸机的
表现；Redis 测试也是 LXC 明显占优，有的测试项目中甚至请求数达到了 KVM 的一倍；Mysql 测试
中，LXC 以 $10\%$ 左右的劣势落后于 KVM ，但是这个劣势相对于前两项对 KVM 的优势来说是
非常微小的。

\subsubsection{解压缩}

tar/xz 的测试结果如表~\ref{tab:tar-xz-perf}所示：

\begin{table}[H]
    \centering
    \caption{Tar/xz 的性能测试结果}
    \begin{tabular}{||c c c c||}
        \hline
        FIELD & KVM & LXC & BAREMETAL \\
        \hline
        \hline
        Real (s) & 16.195 & 21.424 & 16.948 \\
        \hline
        User (s) & 11.509 & 11.053 & 11.167 \\
        \hline
        Sys (s) & 4.642 & 5.932 & 5.493 \\
        \hline
    \end{tabular}
    \label{tab:tar-xz-perf}
\end{table}

这项测试中 LXC 以 $30\%$ 左右的劣势落后于 KVM 。

\subsection{不同虚拟化方案小结}

LXC 虚拟容器在 Web 服务测试中和 KVM 表现非常接近，而且它们的性能都接近裸机，性能很好；
在存储和数据库应用中，LXC 有非常明显的优势，KVM 性能表现不佳；而在解压缩测试中，LXC
反而有劣势，但是劣势不如存储应用中对 KVM 的优势明显。

当然，QEMU-KVM 与 GNU/Linux 内核结合紧密，已经是一种非常非常高效的虚拟化方案了，
据现就职于 Rackspace 的工程师 Major Hayden 在 2014 年进行的测试~\cite{kvm-vs-xen}，
KVM 几乎在每个项目中击败了 Xen （当然 Xen 本身也是一种非常高效的虚拟机管理程序）。
LXC 虚拟容器技术甚至能在一些测试中击败 KVM ，证明它有自己的优势。在 Tsinghua NOVA 系统中，
我们正是要将二者的优势结合起来，取长补短，构建混合的云集群。这样才能最好地发挥现今不同虚拟化
技术在不同应用场景下的优势。

\section{调度性能评估}

\subsection{实验环境}

实验环境包含三台由千兆以太网交换机相连的机架式服务器，它们的匹配如下：

\begin{lstlisting}
Dell PowerEdge R720
|-- Dual Xeon E5-2640 v2
|-- 32GB DDR3 ECC SDRAM
|-- CentOS 7.2 x86_64 (Linux 3.10.0)
|-- Oracle Java SE 1.7.0_79
|-- Libvirt 1.2.17
\end{lstlisting}

其中一台作为主节点 (master node) ，另外两台作为从节点 (slave node) 。主节点还作为另外
两者的网关，从节点不运行防火墙，主节点运行 iptables 服务作为防火墙，关闭了 CentOS 默认
使用的 firewalld 。从节点所有 IPV4 流量需要经过主节点转发。

\subsection{创建客户机开销}

通过主节点 (master node) 和从节点 (worker node) 的日志分析创建客户机的性能开销：

\begin{lstlisting}
2016-06-11 18:31:38,388: entering create vnode handler
2016-06-11 18:31:38,388: hypervisor to be created: lxc
2016-06-11 18:31:38,615: Pnode ip addr: 192.168.7.102
2016-06-11 18:31:38,626: nfs base dir: /home/export/Nova_home/run
2016-06-11 18:31:38,627: extract cmd: tar -C /home/export/Nova_home/run/testlxc8 -xf /home/export/Nova_home/run/fedora-archive-criu-tmux.tar
\end{lstlisting}

\begin{lstlisting}
2016-06-11 18:31:40,780: entering start vnode handler
2016-06-11 18:31:40,781: using nfs as storage protocol
2016-06-11 18:31:40,781: hypervisor to start: lxc:///
2016-06-11 18:31:41,305: domain name: testlxc8; id: 22641
2016-06-11 18:31:41,306: uuid: ae5fc3ae-10a9-454a-932d-4c5114125fcd; vncport: null
\end{lstlisting}

可以看出，不到 3 秒集群完成了整个客户机的创建工作，其中，展开文件系统与 RPC 只花费了 2 秒，
可以说客户机创建过程性能是令人满意的。

\subsection{迁移开销}

在迁移的实验中，我们使用理想化的负载 toy.py ，它的作用是不断把内存中的一个计数变量写入磁盘
块设备中的文件 testfile ，通过查看 testfile 中的数字可以判断实时迁移 (live migration)
是否成功。

\begin{lstlisting}[language=Python]
#!/usr/bin/python2.7
import time

data = 42

while(True):
    with open("testfile", "w+") as f:
        f.write(str(data) + '\n')
    data += 1
    time.sleep(10)
\end{lstlisting}

测试迁移开销的方法是通过输出日志的方式。日志是通过 log4j 打印在 \$NOVA\_HOME/data/log 中
的相应的 log 文件的。

\subsubsection{非实时迁移的开销}

一次非实时迁移 (offline migration) 的结果如下：

\begin{lstlisting}
2016-06-10 23:27:57,606: uuid of domain to migrate is b2aebf27-1b30-4f81-a4cf-62c6682409c9
2016-06-10 23:27:57,606: migration type: offline
2016-06-10 23:28:00,329: dst domain created.
2016-06-10 23:28:00,329: send back migration complete message to master!
2016-06-10 23:28:00,331: migration completed. check process on destination.
\end{lstlisting}

可以看出，非实时迁移，也就是离线迁移是非常快的，整个过程的开销不到 3 秒。

\subsubsection{实时迁移的开销}

一次实时迁移 (live migration) 的结果如下：

\begin{lstlisting}
2016-05-05 21:08:43,693: uuid of domain to migrate is 30680a29-215a-48a6-b728-5643fb7d7dc7
2016-05-05 21:08:43,693: checkpoint! container name: testlxc4; process name: toy.py; ip addr: 192.168.122.4
2016-05-05 21:08:43,694: cmd: /home/Nova/lxc-cr-wrapper checkpoint -c testlxc4 -p toy.py -i 192.168.122.4
2016-05-05 21:08:54,009: dst domain created.
2016-05-05 21:08:54,009: restoring process in destination domain...
2016-05-05 21:08:54,009: restore! dst ip: 192.168.7.101; container name: testlxc4; process name: toy.py; ip addr: 192.168.122.4
2016-05-05 21:08:54,010: cmd: ssh 192.168.7.101 /home/Nova/lxc-cr-wrapper restore -c testlxc4 -p toy.py -i 192.168.122.4
2016-05-05 21:09:21,294: restore succeeded!
\end{lstlisting}

可以看出，在源宿主机上的虚拟容器中 checkpoint 负载进程需要大约 10 秒，而在目标宿主机上
迁移过后的虚拟容器中 restore 负载进程需要 27 秒左右。整个过程存在大约 40 秒的开销。

\subsection{调度性能小结}

从以上的实验可以看出，目前新的 LXC 客户机创建和离线迁移速度已经能满足实际要求，但是实时迁移
速度还不够理想。从设计方法上看，这主要是因为如下几个原因：

\begin{enumerate}
    \item 最大的影响因素是目前与容器内部实现通讯使用的是 ssh 的方式，它的开销非常大。
    比较理想的 RPC 方式是在客户操作系统内安装一个可以负责这些操作的 agent ；
    \item 其次是容器在迁移前后要开关机一次，而重新开机之后会有一段时间 sshd 还不能接受
    连接请求；
    \item 再次现在的 lxc-cr-wrapper 脚本写的不紧凑，如果能减少 ssh login 的次数，
    性能也会有一定的提高。
\end{enumerate}

\section{小结}

通过上述的比较可以看出，虚拟容器技术和传统虚拟化的新秀 QEMU-KVM 的性能表现可以说各有千秋，
而 Tsinghua NOVA 系统可以创建二者混合的集群，这样可以扬长避短，发挥二者各自的优势——如果
该应用程序的类型被基准测试程序 Omegabench 验证过在典型负载下适合在 LXC 中运行，那么就使用
LXC 作为下层的虚拟化方案，如果反之，是在 QEMU-KVM 里效果更好，那么就使用 KVM 作为下层的
虚拟化方案。而采用不同虚拟化方案产生的客户机被 Tsinghua NOVA 使用透明的方式呈现给客户，
并没有增加用户的使用和学习成本。
